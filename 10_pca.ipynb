{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "by Marc Deisenroth and Yicheng Luo\n",
    "\n",
    "We will implement the PCA algorithm using the projection perspective. We will first implement PCA, then apply it to the MNIST digit dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "1. Write code that implements PCA.\n",
    "2. Write code that implements PCA for high-dimensional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the packages we need for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT THIS CELL\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from ipywidgets import interact\n",
    "#from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT THIS CELL\n",
    "from sklearn.datasets import fetch_openml\n",
    "images, labels = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
      "   18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
      "  253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
      "  253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
      "  198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
      "   11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
      "    2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
      "   70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
      "  225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
      "  240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
      "  229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
      "  253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
      "  253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
      "   80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot a digit from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(images.iloc[0].values.reshape(28, 28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement PCA, we will need to do some data preprocessing. In this assessment, some of them \n",
    "will be implemented by you, others we will take care of. However, when you are working on real world problems, you will need to do all these steps by yourself.\n",
    "\n",
    "The preprocessing steps we will do are\n",
    "1. Convert unsigned interger 8 (uint8) encoding of pixels to a floating point number between 0 and 1.\n",
    "2. Subtract from each image the mean $\\boldsymbol \\mu$.\n",
    "3. Scale each dimension of each image by $\\frac{1}{\\sigma}$ where $\\sigma$ is the stardard deviation.\n",
    "\n",
    "The steps above ensure that our images will have zero mean and one variance. These preprocessing\n",
    "steps are also known as [Data Normalization or Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA\n",
    "\n",
    "Now we will implement PCA. Before we do that, let's pause for a moment and\n",
    "think about the steps for performing PCA. Assume that we are performing PCA on\n",
    "some dataset $\\boldsymbol X$ for $M$ principal components. \n",
    "We then need to perform the following steps, which we break into parts:\n",
    "\n",
    "1. Data normalization (`normalize`).\n",
    "2. Find eigenvalues and corresponding eigenvectors for the covariance matrix $S$.\n",
    "   Sort by the largest eigenvalues and the corresponding eigenvectors (`eig`).\n",
    "\n",
    "After these steps, we can then compute the projection and reconstruction of the data onto the spaced spanned by the top $n$ eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"Normalize the given dataset X\n",
    "    Args:\n",
    "        X: ndarray, dataset\n",
    "    \n",
    "    Returns:\n",
    "        (Xbar, mean, std): tuple of ndarray, Xbar is the normalized dataset\n",
    "        with mean 0 and standard deviation 1; mean and std are the \n",
    "        mean and standard deviation respectively.\n",
    "    \n",
    "    Note:\n",
    "        You will encounter dimensions where the standard deviation is\n",
    "        zero, for those when you do normalization the normalized data\n",
    "        will be NaN. Handle this by setting using `std = 1` for those \n",
    "        dimensions when doing normalization.\n",
    "    \"\"\"\n",
    "    mu = X.mean(axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    std_filled = std.copy()\n",
    "    std_filled[std==0] = 1.\n",
    "    Xbar = (X - mu)/std_filled\n",
    "    return Xbar, mu, std\n",
    "\n",
    "def eig(S):\n",
    "    \"\"\"Compute the eigenvalues and corresponding eigenvectors \n",
    "        for the covariance matrix S.\n",
    "    Args:\n",
    "        S: ndarray, covariance matrix\n",
    "    \n",
    "    Returns:\n",
    "        (eigvals, eigvecs): ndarray, the eigenvalues and eigenvectors\n",
    "\n",
    "    Note:\n",
    "        the eigenvals and eigenvecs should be sorted in descending\n",
    "        order of the eigen values\n",
    "    \"\"\"\n",
    "    # To obtain eigenvalues and eigenvectors we resolve the equation det(S - lambdaÂ·I) = 0\n",
    "    # Because S is a simmetric matrix, it is assured the eigenvalues of S will make up a \n",
    "    # ONB of the space.\n",
    "    # Here we use numpys implementation to retrieve both the eigenvalues and the eigenvectors\n",
    "    return np.linalg.eig(S)\n",
    "\n",
    "def projection_matrix(B):\n",
    "    \"\"\"Compute the projection matrix onto the space spanned by `B`\n",
    "    Args:\n",
    "        B: ndarray of dimension (D, M), the basis for the subspace\n",
    "    \n",
    "    Returns:\n",
    "        P: the projection matrix\n",
    "    \"\"\"\n",
    "    return np.eye(B.shape[0]) # <-- EDIT THIS to compute the projection matrix\n",
    "\n",
    "def PCA(X, num_components):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of size (N, D), where D is the dimension of the data,\n",
    "           and N is the number of datapoints\n",
    "        num_components: the number of principal components to use.\n",
    "    Returns:\n",
    "        X_reconstruct: ndarray of the reconstruction\n",
    "        of X from the first `num_components` principal components.\n",
    "    \"\"\"\n",
    "    evalues, evectors = eig(X.T @ X)\n",
    "    ordered_idx = np.argsort(evalues)[-num_component]\n",
    "    \n",
    "    # your solution should take advantage of the functions you have implemented above.\n",
    "    return X # <-- EDIT THIS to return the reconstruction of X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some preprocessing of the data\n",
    "NUM_DATAPOINTS = 1000\n",
    "X = (images.values.reshape(-1, 28 * 28)[:NUM_DATAPOINTS]) / 255.\n",
    "Xbar, mu, std = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[592 593 596 597 598 602 601 603 607 662 663 664 783 666 661 667 665 660\n",
      " 655 658 657 656 668 654 653 652 651 659 669 674 671 688 687 686 685 684\n",
      " 683 682 681 680 679 678 677 676 675 650 673 672 670 649 643 647 624 623\n",
      " 622 621 620 619 618 617 616 615 614 613 612 611 610 609 782 625 648 626\n",
      " 628 646 644 689 642 641 640 639 638 637 636 635 634 633 632 631 630 629\n",
      " 627 690 645 692 757 756 755 754 753 752 751 750 749 748 747 746 745 744\n",
      " 743 742 741 740 739 758 759 760 761 781 780 779 778 777 691 775 774 773\n",
      " 738 772 770 769 768 767 766 765 764 763 762 771 737 776 697 712 711 710\n",
      " 709 708 707 706 705 736 703 702 701 700 699 698 696 695 694 693 713 714\n",
      " 704 716 735 734 733 732 715 730 729 728 727 731 719 725 717 718 726 724\n",
      " 723 722 720 721 608 605 604 606 600 599 595 594 591 590 497 482 477 471\n",
      " 470 469 468 334 367 450 451 453 452 368 473 454 472 474 475 476 478 479\n",
      " 480 481 483 484 485 487 495 498 499 500 501 525 502 524 509 508 518 533\n",
      " 510 534 536 542 539 541 548 545 547 546 540 557 580 556 554 558 566 583\n",
      " 571 564 565 563 567 573 588 568 575 572 579 586 581 589 577 574 587 576\n",
      " 570 569 578 562 582 561 585 584 560 552 553 551 559 555 550 549 538 544\n",
      " 543 537 535 532 531 527 530 526 529 528 520 523 521 522 519 514 513 517\n",
      " 515 516 512 511 507 505 506 503 504 493 496 488 489 494 490 491 492 486\n",
      " 463 455 459 462 465 464 467 458 457 460 461 456 438 439 440 441 466 448\n",
      " 446 447 444 442 443 449 445 402 403 404 410 412 411 413 414 418 436 419\n",
      " 420 421 423 430 433 432 434 431 437 435 427 428 429 424 422 426 425 417\n",
      " 416 415 409 408 407 406 405 401 395 397 400 399 398 396 393 394 392 391\n",
      " 387 388 389 390 386 385 384 383 380 381 382 379 378 377 375 374 376 373\n",
      " 372 371 370 369 366 365 364 360 363 362 361 359 358 357 356 355 354 353\n",
      " 352 350 351 349 348 347 346 345 344 343 341 342 340 338 339 337 336 335\n",
      " 333 332 331 330 328 329 327 326 324 323 325 322 321 320 319 318 317 316\n",
      " 315 314 313 312 311 295 298 307 302 309 304 310 308 303 301 305 306 299\n",
      " 296 300 297 292 293 294 291 290 289 288 287 286 285 284 281 282 283 280\n",
      " 279 278 277 276 275 274 273 272 271 270 269 268 267 264 266 265 263 262\n",
      " 261 259 260 258 257 256 255 254 242 252 253 251 249 247 248 246 250 241\n",
      " 238 239 240 243 245 244 237 233 234 235 236 232 231 230 229 228 227 226\n",
      " 225 222 223 221 224 220 219 216 218 215 214 217 213 212 211 210 209 208\n",
      " 207 206 205 202 204 203 201 199 200 198 197 196 194 195 192 193 191 190\n",
      " 189 188 187 186 179 182 181 185 184 183 178 175 180 177 176 174 173 172\n",
      " 171 170 169 168 167 166 164 165 163 162 161 160 159 158 157 156 155 152\n",
      " 154 153 151 150 148 149 147 146 145 144 143 142 141 139 140 138 137 136\n",
      " 132 133 135 134 131 130 128 129 127 123 124 125 126 122 121 120 119 118\n",
      " 117 116 115 114 107 113 109 110 111 112 108 105 106 103 104  99 102 101\n",
      " 100  98  97  96  95  93  92  94  90  91  89  88  87  86  85  83  84  78\n",
      "  79  80  81  82  77  76  75  74  73  72  71  70  69  68  67  66  65  63\n",
      "  62  64  61  59  60  58  57  56  55  54  53  52  51  50  49  48  47  46\n",
      "  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29  28\n",
      "  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11  10\n",
      "   9   8   7   6   5   4   3   2   1   0]\n"
     ]
    }
   ],
   "source": [
    "#for num_component in range(1, 20):\n",
    "for num_component in range(1, 2):\n",
    "    from sklearn.decomposition import PCA as SKPCA\n",
    "    # We can compute a standard solution given by scikit-learn's implementation of PCA\n",
    "    pca = SKPCA(n_components=num_component, svd_solver='full')\n",
    "    sklearn_reconst = pca.inverse_transform(pca.fit_transform(Xbar))\n",
    "    reconst = PCA(Xbar, num_component)\n",
    "    #np.testing.assert_almost_equal(reconst, sklearn_reconst)\n",
    "    #print(np.square(reconst - sklearn_reconst).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater number of of principal components we use, the smaller will our reconstruction\n",
    "error be. Now, let's answer the following question: \n",
    "\n",
    "\n",
    "> How many principal components do we need\n",
    "> in order to reach a Mean Squared Error (MSE) of less than $100$ for our dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a function in the next cell that computes the mean squared error (MSE), which will be useful for answering the question above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predict, actual):\n",
    "    \"\"\"Helper function for computing the mean squared error (MSE)\"\"\"\n",
    "    return np.square(predict - actual).sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "reconstructions = []\n",
    "# iterate over different numbers of principal components, and compute the MSE\n",
    "for num_component in range(1, 100):\n",
    "    reconst = PCA(Xbar, num_component)\n",
    "    error = mse(reconst, Xbar)\n",
    "    reconstructions.append(reconst)\n",
    "    # print('n = {:d}, reconstruction_error = {:f}'.format(num_component, error))\n",
    "    loss.append((num_component, error))\n",
    "\n",
    "reconstructions = np.asarray(reconstructions)\n",
    "reconstructions = reconstructions * std + mu # \"unnormalize\" the reconstructed image\n",
    "loss = np.asarray(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# create a table showing the number of principal components and MSE\n",
    "pd.DataFrame(loss).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also put these numbers into perspective by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss[:,0], loss[:,1]);\n",
    "ax.axhline(100, linestyle='--', color='r', linewidth=2)\n",
    "ax.xaxis.set_ticks(np.arange(1, 100, 5));\n",
    "ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs number of principal components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But _numbers dont't tell us everything_! Just what does it mean _qualitatively_ for the loss to decrease from around\n",
    "$450.0$ to less than $100.0$?\n",
    "\n",
    "Let's find out! In the next cell, we draw the the leftmost image is the original dight. Then we show the reconstruction of the image on the right, in descending number of principal components used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact(image_idx=(0, 1000))\n",
    "def show_num_components_reconst(image_idx):\n",
    "    fig, ax = plt.subplots(figsize=(20., 20.))\n",
    "    actual = X[image_idx]\n",
    "    # concatenate the actual and reconstructed images as large image before plotting it\n",
    "    x = np.concatenate([actual[np.newaxis, :], reconstructions[:, image_idx]])\n",
    "    ax.imshow(np.hstack(x.reshape(-1, 28, 28)[np.arange(10)]),\n",
    "              cmap='gray');\n",
    "    ax.axvline(28, color='orange', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also browse throught the reconstructions for other digits. Once again, `interact` becomes handy for visualing the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@interact(i=(0, 10))\n",
    "def show_pca_digits(i=1):\n",
    "    \"\"\"Show the i th digit and its reconstruction\"\"\"\n",
    "    plt.figure(figsize=(4,4))\n",
    "    actual_sample = X[i].reshape(28,28)\n",
    "    reconst_sample = (reconst[i, :] * std + mu).reshape(28, 28)\n",
    "    plt.imshow(np.hstack([actual_sample, reconst_sample]), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA for high-dimensional datasets\n",
    "\n",
    "Sometimes, the dimensionality of our dataset may be larger than the number of samples we\n",
    "have. Then it might be inefficient to perform PCA with our implementation above. Instead,\n",
    "we can implement PCA in a more efficient manner, which we call \"PCA for high dimensional data\" (PCA_high_dim).\n",
    "\n",
    "Below are the steps for performing PCA for high dimensional dataset\n",
    "1. Compute the matrix $\\boldsymbol X\\boldsymbol X^T$ (a $N$ by $N$ matrix with $N \\ll D$)\n",
    "2. Compute eigenvalues $\\lambda$s and eigenvectors $V$ for $\\boldsymbol X\\boldsymbol X^T$\n",
    "3. Compute the eigenvectors for the original covariance matrix as $\\boldsymbol X^T\\boldsymbol V$. Choose the eigenvectors associated with the M largest eigenvalues to be the basis of the principal subspace $U$.\n",
    "4. Compute the orthogonal projection of the data onto the subspace spanned by columns of $\\boldsymbol U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA for high-dimensional datasets\n",
    "\n",
    "def PCA_high_dim(X, n_components):\n",
    "    \"\"\"Compute PCA for small sample size but high-dimensional features. \n",
    "    Args:\n",
    "        X: ndarray of size (N, D), where D is the dimension of the sample,\n",
    "           and N is the number of samples\n",
    "        num_components: the number of principal components to use.\n",
    "    Returns:\n",
    "        X_reconstruct: (N, D) ndarray. the reconstruction\n",
    "        of X from the first `num_components` pricipal components.\n",
    "    \"\"\"\n",
    "    return X # <-- EDIT THIS to return the reconstruction of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the same dataset, `PCA_high_dim` and `PCA` should give the same output. \n",
    "Assuming we have implemented `PCA`, correctly, we can then use `PCA` to test the correctness\n",
    "of `PCA_high_dim`. Given the same dataset, `PCA` and `PCA_high_dim` should give identical results.\n",
    "\n",
    "We can use this __invariant__\n",
    "to test our implementation of PCA_high_dim, assuming that we have correctly implemented `PCA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(PCA(Xbar, 2), PCA_high_dim(Xbar, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the running time between `PCA` and `PCA_high_dim`.\n",
    "\n",
    "__Tips__ for running benchmarks or computationally expensive code:\n",
    "\n",
    "When you have some computation that takes up a non-negligible amount of time. Try separating\n",
    "the code that produces output from the code that analyzes the result (e.g. plot the results, compute statistics of the results). In this way, you don't have to recompute when you want to produce more analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell includes a function that records the time taken for executing a function `f` by repeating it for `repeat` number of times. You do not need to modify the function but you can use it to compare the running time for functions which you are interested in knowing the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time(f, repeat=10):\n",
    "    times = []\n",
    "    for _ in range(repeat):\n",
    "        start = timeit.default_timer()\n",
    "        f()\n",
    "        stop = timeit.default_timer()\n",
    "        times.append(stop-start)\n",
    "    return np.mean(times), np.std(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first benchmark the time taken to compute $\\boldsymbol X^T\\boldsymbol X$ and $\\boldsymbol X\\boldsymbol X^T$. Jupyter's magic command `%time` is quite handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell finds the running time for computing $\\boldsymbol X^T\\boldsymbol X$ and $\\boldsymbol X\\boldsymbol X^T$ for different dimensions of $\\boldsymbol X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_mm0 = []\n",
    "times_mm1 = []\n",
    "\n",
    "# iterate over datasets of different size\n",
    "for datasetsize in np.arange(4, 784, step=20):\n",
    "    XX = Xbar[:datasetsize] # select the first `datasetsize` samples in the dataset\n",
    "    # record the running time for computing X.T @ X\n",
    "    mu, sigma = time(lambda : XX.T @ XX)\n",
    "    times_mm0.append((datasetsize, mu, sigma))\n",
    "    \n",
    "    # record the running time for computing X @ X.T\n",
    "    mu, sigma = time(lambda : XX @ XX.T)\n",
    "    times_mm1.append((datasetsize, mu, sigma))\n",
    "    \n",
    "times_mm0 = np.asarray(times_mm0)\n",
    "times_mm1 = np.asarray(times_mm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having recorded the running time for computing `X @ X.T` and `X @ X.T`, we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='size of dataset', ylabel='running time')\n",
    "bar = ax.errorbar(times_mm0[:, 0], times_mm0[:, 1], times_mm0[:, 2], label=\"$X^T X$ (PCA)\", linewidth=2)\n",
    "ax.errorbar(times_mm1[:, 0], times_mm1[:, 1], times_mm1[:, 2], label=\"$X X^T$ (PCA_high_dim)\", linewidth=2)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, use the `time` magic command for benchmarking functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time Xbar.T @ Xbar\n",
    "%time Xbar @ Xbar.T\n",
    "pass # Put this here so that our output does not show result of computing `Xbar @ Xbar.T`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we benchmark PCA, PCA_high_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times0 = []\n",
    "times1 = []\n",
    "\n",
    "# iterate over datasets of different size\n",
    "for datasetsize in np.arange(4, 784, step=100):\n",
    "    XX = Xbar[:datasetsize]\n",
    "    npc = 2\n",
    "    mu, sigma = time(lambda : PCA(XX, npc), repeat=10)\n",
    "    times0.append((datasetsize, mu, sigma))\n",
    "    \n",
    "    mu, sigma = time(lambda : PCA_high_dim(XX, npc), repeat=10)\n",
    "    times1.append((datasetsize, mu, sigma))\n",
    "    \n",
    "times0 = np.asarray(times0)\n",
    "times1 = np.asarray(times1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the running time. Spend some time and think about what this plot means. We mentioned in lectures that PCA_high_dim are advantageous when\n",
    "we have dataset size $N$ < data dimension $M$. Although our plot does not for the two running time does not intersect exactly at $N = M$, it does show the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='number of datapoints', ylabel='run time')\n",
    "ax.errorbar(times0[:, 0], times0[:, 1], times0[:, 2], label=\"PCA\", linewidth=2)\n",
    "ax.errorbar(times1[:, 0], times1[:, 1], times1[:, 2], label=\"PCA_high_dim\", linewidth=2)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, with the magic command `time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time PCA(Xbar, 2)\n",
    "%time PCA_high_dim(Xbar, 2)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "mathematics-machine-learning-pca",
   "graded_item_id": "CXC11",
   "launcher_item_id": "ub5A7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
